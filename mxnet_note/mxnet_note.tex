\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{mxnet笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

\section{NDArray}

\noindent \texttt{from mxnet import nd}\\
\texttt{x = nd.arange(12)} // 创建一个长度为12的行向量，类型为NDArray 12\\
\texttt{x.shape} // 返回(m, n)，代表x为 m行n列 矩阵。对于向量，行数或列数不存在\\
\texttt{x.size} // 返回矩阵中元素个数\\
\texttt{x.reshape((m', n'))}

  更改x的shape，元素按行填写进新张量。如果 n' * m' < 原元素数，多余元素被舍弃。如果n' * m' > 原元素数，报错

  当二次resize，使用与开始定义x的size，而非上一次resize后舍弃部分值的x.size\\
\texttt{x.reshape((-1, n')) x.reshape((m', -1))}
 
  当m', n' = -1，m' = $\lfloor \frac{x.size}{n'}\rfloor $，n'同理

  当m', n'为空，reshape成向量\\
\texttt{nd.zeros(($v_1, v_2, v_3, ..., v_n$))}
  
  创建一个张量，类型仍为NDArray。有$v_1$个子张量，每个子张量分别有$v_2$个子张量。最后一层张量有$v_n$元素，每一元素都为0。
  
  张量同样可以reshape，reshape结果只能是矩阵或向量\\
\texttt{nd.ones(($v_1, v_2, v_3, ..., v_n$))} // 所有元素为1的张量\\
\texttt{nd.array([...])}
 
  得到python list类型的矩阵，返回NDArray类型的矩阵
  
  输入可以是python常数，但随后计算会报错\\
\texttt{nd.random.normal($\mu, \sigma$， shape=($v_1, v_2, v_3, ..., v_n$))}

  随机生成张量，元素值$\sim N(\mu, \sigma)$\\\\
\texttt{X + - * / Y}

  张量element-wise操作
  
  当X Y维数不同时，广播boardcast 机制先将X，Y按行或列复制成维数一样的张量，随后element-wise操作\\
\texttt{X.exp()} // 张量element-wise 取指数\\
\texttt{nd.dot(X, Y)} //矩阵乘法\\
\texttt{nd.concat(X, Y, dim=n)}

  在第n纬度将矩阵concat，除此纬度其余所有纬度必须完全一样\\
\texttt{X == Y} 

  elementwise比较张量元素，纬度必须相同\\
\texttt{X.sum()} // 所有元素和\\
\texttt{X.sum(axis=n, keepdims=True/False)}

  对张量第n维的数据求和
  
  keepdims时shape转换：$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$ -> $v_1, v_2, ..., v_{n-1}, 1, v_{n+1}, ...$

  否则，shape转换：$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$ -> $v_1, v_2, ..., v_{n-1}, v_{n+1}, ...$

  例：n=0，对列求和 n=1，对行求和\\
\texttt{X.argmax(axis=n, keepdims=Ture/False)}
  
  找到给定维度中最大元素的index，shape转换同上

  算法：对张量$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$

  \quad 结果张量包括一个$v_1, v_2, ..., v_{n-1}$张量

  \quad 每一元素张量为原张量$v_n$维中所有张量的element wise比较结果。shape为$v_{n+1}, ...$，其中每一元素为一index，标记$v_n$维中此位置最大元素值所在的张量index\\
\texttt{X.norm()}
 
  得到仅包含一元素的矩阵，元素值为2-norm

  可以对张量取2-norm\\
\texttt{X.asscalar()} // 如果X仅包含一元素，输出此元素值\\
\texttt{X[$v_1, v_2, v_3, ..., v_n$]}

  index取值操作，同X$[v_1][v_2][v_3]...[v_n]$

  当$v_i$为n:m时，代表范围$[n, m)$\\
\texttt{X.asnumpy()} // 转换成python list\\\\
\texttt{from d2lzh import plt}\\
\texttt{plt.scatter(array\_x, array\_y, 1)}

  描点，\texttt{array\_x}, \texttt{array\_y}为python list


\section{训练}

\noindent \texttt{from mxnet import autograd}\\
\texttt{x.attach\_grad()} // 为自变量x的 $\frac{d }{d x} $项分配内存\\
\texttt{with autograd.record():}

  \texttt{因变量 = 关于x的表达式}
  
  - 关于x的表达式可以为一个自定义function，不需要是一个连续的数学函数
  
  - 自定义函数必须将所有使用的变量包括在def输入变量中，不可使用全局变量。const仍可使用全局变量
  
  - 当变量为另一函数的结果，计算另一函数的步骤需放进with 中，不能在with scope外计算完with内调取\\
\texttt{因变量.backward()}
  
  - 定义x的表达式，并计算表达式在x内每一元素值上的斜率，对应斜率矩阵存在x.attach\_grad()分配的内存中
  
  - 当使用多组sample，因变量为向量。此时backward等同于\texttt{因变量.sum().backward()}\\
\texttt{x.grad} // 调取斜率矩阵\\
\texttt{autograd.is\_training()} // 在\texttt{autograd.record()内返回true，否则返回false}


\section{使用neural network模型训练}

\noindent \texttt{from mxnet.gluon import nn}\\
\texttt{from mxnet import init}\\
\texttt{from mxnet.gluon import loss as gloss}\\
\texttt{net = nn.Sequential()} // 创建一个神经网络模型，不包含任何layer\\
\texttt{net.add(nn.Dense(2))} // 在模型中加入一个全连接层，包含一个节点\\
\texttt{net.initialize(init.Normal(sigma=0.3))} // 初始化整个神经网络，所有权重$\sim N(0, 0.3)$，所有偏差值=0\\
\texttt{loss = gloss.L2Loss()}
  
  定义损失函数为平方损失函数，loss为一函数\\
\texttt{trainer = gluon.Trainer(net.collect\_params(NAME), 'sgd', {'learning\_rate': 0.03}, WD)}

  定义每一步优化函数，使用sgd梯度下降
  
  NAME无定义，则同时训练权重
  
  \quad NAME = \texttt{'.*weight'}，只训练权重 
  
  \quad NAME = \texttt{'.*bias'}，只训练偏差
    
  WD无定义，则不使用权重衰减
  
  \quad WD = \texttt{wd:'wd'}，使用权重衰减\\
\texttt{dataset = gdata.ArrayDataset(features, labels)}\\
\texttt{data\_iter = gdata.DataLoader(dataset, batch\_size, shuffle=True, num\_workers)}

  按批量读取数据，\texttt{num\_workers}代表使用的额外处理器数，0代表没有额外处理器\\
\texttt{trainer.step(batch\_size)}

  调用优化函数，取batch\_size个sample做一步训练\\
\texttt{l = loss(net(features), labels)}

  调用loss函数
\section{图像分类数据集fashion-MINST}
\noindent \texttt{from mxnet.gluon import data as gdata}\\
\texttt{mnist\_train = gdata.version.FashionMINST(train=True)} 

  得到数据集, \texttt{train=True}为训练数据集，否则为测试训练集\\
\texttt{features, label = mnist\_train[i:j]}
  
  得到一个或多个sample
  
  features为(j-i, 28, 28, 1)的张量，类型为\texttt{NDArray}\\
\texttt{from d2lzh import show\_fashion\_mnist}\\
\texttt{show\_fashion\_mnist(features, ['label1', 'label2'])}

  打印图片，显示图片仍使用plt.show()

  features必须包含多余一个图片矩阵，否则报错




\end{document}
