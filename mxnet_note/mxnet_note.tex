\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{mxnet笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

\section{NDArray}

\noindent \texttt{from mxnet import nd}\\
\texttt{x = nd.arange(12)} // 创建一个长度为12的行向量，类型为NDArray 12\\
\texttt{x.shape} // 返回(m, n)，代表x为 m行n列 矩阵。对于向量，行数或列数不存在\\
\texttt{x.size} // 返回矩阵中元素个数\\
\texttt{x.reshape((m', n'))}

  更改x的shape，元素按行填写进新矩阵/向量。如果 n' * m' < 原元素数，多余元素被舍弃。如果n' * m' > 原元素数，报错

  当二次resize，使用与开始定义x的size，而非上一次resize后舍弃部分值的x.size
  
  只能reshape成矩阵或向量，不能reshape成张量\\
\texttt{x.reshape((-1, n')) x.reshape((m', -1))}
 
  当m', n' = -1，m' = $\lfloor \frac{x.size}{n'}\rfloor $，n'同理

  当m', n'为空，reshape成向量\\
\texttt{nd.zeros(($v_1, v_2, v_3, ..., v_n$))}
  
  创建一个张量，类型仍为NDArray。有$v_1$个子张量，每个子张量分别有$v_2$个子张量。最后一层张量有$v_n$元素，每一元素都为0。
  
  张量同样可以reshape，reshape结果只能是矩阵或向量\\
\texttt{nd.ones(($v_1, v_2, v_3, ..., v_n$))} // 所有元素为1的张量\\
\texttt{nd.array([...])}
 
  得到python list类型的矩阵，返回NDArray类型的矩阵
  
  输入可以是python常数，但随后计算会报错\\
\texttt{nd.random.normal($\mu, \sigma$， shape=($v_1, v_2, v_3, ..., v_n$))}

  随机生成张量，元素值$\sim N(\mu, \sigma)$\\\\
\texttt{X + - * / Y}

  张量element-wise操作
  
  当X Y维数不同时，广播boardcast 机制先将X，Y按行或列复制成维数一样的张量，随后element-wise操作\\
\texttt{X.exp()} // 张量element-wise 取指数\\
\texttt{nd.dot(X, Y)} //矩阵乘法\\
\texttt{nd.concat(X, Y, dim=n)}

  在第n纬度将矩阵concat，除此纬度其余所有纬度必须完全一样\\
\texttt{X == Y} 

  elementwise比较张量元素，纬度必须相同\\
\texttt{X.sum()} // 所有元素和\\
\texttt{X.norm()}
 
  得到仅包含一元素的矩阵，元素值为2-norm

  可以对张量取2-norm\\
\texttt{X.asscalar()} // 如果X仅包含一元素，输出此元素值\\
\texttt{X[$v_1, v_2, v_3, ..., v_n$]}

  index取值操作，同X$[v_1][v_2][v_3]...[v_n]$

  当$v_i$为n:m时，代表范围$[n, m)$\\
\texttt{X.asnumpy()} // 转换成python list


\section{训练}

\noindent \texttt{from mxnet import autograd}\\
\texttt{x.attach\_grad()} // 为自变量x的 $\frac{d }{d x} $项分配内存\\
\texttt{with autograd.record():}

  \texttt{因变量 = 关于x的表达式}
  
  - 关于x的表达式可以为一个自定义function，不需要是一个连续的数学函数
  
  - 自定义函数必须将所有使用的变量包括在def输入变量中，不可使用全局变量。const仍可使用全局变量
  
  - 当变量为另一函数的结果，需将另一函数inline进传入参数步骤，不能使用局部变量传入\\
\texttt{因变量.backward()}
  
  - 定义x的表达式，并计算表达式在x内每一元素值上的斜率，对应斜率矩阵存在x.attach\_grad()分配的内存中
  
  - 当使用多组sample，因变量为向量。此时backward等同于\texttt{因变量.sum().backward()}\\
\texttt{x.grad} // 调取斜率矩阵\\
\texttt{autograd.is\_training()} // 在\texttt{autograd.record()内返回true，否则返回false}


\section{使用neural network模型训练}

\noindent \texttt{from mxnet.gluon import nn}\\
\texttt{from mxnet import init}\\
\texttt{from mxnet.gluon import loss as gloss}\\
\texttt{net = nn.Sequential()} // 创建一个神经网络模型，不包含任何layer\\
\texttt{net.add(nn.Dense(2))} // 在模型中加入一个全连接层，包含一个节点\\
\texttt{net.initialize(init.Normal(sigma=0.3))} // 初始化整个神经网络，所有权重$\sim N(0, 0.3)$，所有偏差值=0\\
\texttt{loss = gloss.L2Loss()}
  
  定义损失函数为平方损失函数，loss为一函数\\
\texttt{trainer = gluon.Trainer(net.collect\_params(), 'sgd', {'learning\_rate': 0.03})}

  定义每一步优化函数，使用sgd梯度下降\\
\texttt{trainer.step(batch\_size)}

  调用优化函数，取batch\_size个sample做一步训练\\
\texttt{l = loss(net(features), labels)}

  调用loss函数\\



\end{document}
