\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\geometry{a4paper,scale=0.8}
\sectionfont{\bfseries\Large\raggedright}

\title{mxnet笔记}
\author{徐世桐}
\date{}
\begin{document}
\maketitle

% ----------------------------------------------------------------------
% |                              import                                |
% ----------------------------------------------------------------------
\section{imports}
\noindent \texttt{from mxnet import init, image, autograd}\\
\texttt{from mxnet.gluon import nn, model\_zoo, data, loss}\\
\texttt{from d2lzh import show\_bboxes, show\_fashion\_mnist}

% ----------------------------------------------------------------------
% |                              NDArray                               |
% ----------------------------------------------------------------------
\section{NDArray}

\noindent \texttt{from mxnet import nd}\\
\texttt{x = nd.arange(12)} // 创建一个长度为12的行向量，类型为NDArray 12\\
\texttt{x.shape}
  
  返回(m, n)，代表x为 m行n列 矩阵。对于向量，行数或列数不存在

  \textbf{所有(n,)形状点乘时看做列向量，否则和(n, 1)矩阵用法不同}
  
  \textbf{(n, 1)矩阵和(n,)向量计算非元素计算}\\
\texttt{x.size} // 返回矩阵中元素个数\\
\texttt{x.reshape((m', n'))}

  更改x的shape，元素按行填写进新张量。如果 n' * m' < 原元素数，多余元素被舍弃。如果n' * m' > 原元素数，报错

  当二次resize，使用与开始定义x的size，而非上一次resize后舍弃部分值的x.size\\
\texttt{x.reshape((-1, n')) x.reshape((m', -1))}
 
  当m', n' = -1，m' = $\lfloor \frac{x.size}{n'}\rfloor $，n'同理

  当m', n'为空，reshape成向量\\
\texttt{nd.zeros(($v_1, v_2, v_3, ..., v_n$))}
  
  创建一个张量，类型仍为NDArray。有$v_1$个子张量，每个子张量分别有$v_2$个子张量。最后一层张量有$v_n$元素，每一元素都为0。
  
  张量同样可以reshape，reshape结果只能是矩阵或向量\\
\texttt{nd.ones(($v_1, v_2, v_3, ..., v_n$))} // 所有元素为1的张量\\
\texttt{nd.array([...])}
 
  得到python list类型的矩阵，返回NDArray类型的矩阵
  
  输入可以是python常数，但随后计算会报错\\
\texttt{nd.random.normal($\mu, \sigma$, shape=($v_1, v_2, v_3, ..., v_n$))}


  随机生成张量，元素值$\sim N(\mu, \sigma)$\\
\texttt{nd.random.uniform(low, high, shape=($v_1, v_2, v_3, ..., v_n$))}\\\\
\texttt{X + - * / Y}

  张量element-wise操作
  
  当X Y维数不同时，广播boardcast 机制先将X，Y按行或列复制成维数一样的张量，随后element-wise操作\\
\texttt{X.exp()} // 张量element-wise 取指数\\
\texttt{X.relu()} // 对每一X元素使用relu 激发函数\\
\texttt{nd.dot(X, Y)} //矩阵乘法\\
\texttt{nd.concat(X, Y, dim=n)}

  在第n纬度将矩阵concat，除此纬度其余所有纬度必须完全一样\\
\texttt{X == Y} 

  elementwise比较张量元素，纬度必须相同\\
\texttt{X.sum()} // 所有元素和\\
\texttt{X.sum(axis=n, keepdims=True/False)}

  对张量第n维的数据求和
  
  keepdims时shape转换：$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$ -> $v_1, v_2, ..., v_{n-1}, 1, v_{n+1}, ...$

  否则，shape转换：$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$ -> $v_1, v_2, ..., v_{n-1}, v_{n+1}, ...$

  例：n=0，对列求和 n=1，对行求和\\
\texttt{X.argmax(axis=n, keepdims=Ture/False)}
  
  找到给定维度中最大元素的index，shape转换同上

  算法：对张量$v_1, v_2, ..., v_{n-1}, v_n, v_{n+1}, ...$

  \quad 结果张量包括一个$v_1, v_2, ..., v_{n-1}$张量

  \quad 每一元素张量为原张量$v_n$维中所有张量的element wise比较结果。shape为$v_{n+1}, ...$，其中每一元素为一index，标记$v_n$维中此位置最大元素值所在的张量index\\
\texttt{X.norm()}
 
  得到仅包含一元素的矩阵，元素值为2-norm

  可以对张量取2-norm\\
\texttt{X.asscalar()} // 如果X仅包含一元素，输出此元素值\\
\texttt{X[$v_1, v_2, v_3, ..., v_n$]}

  index取值操作，同X$[v_1][v_2][v_3]...[v_n]$

  当$v_i$为n:m时，代表范围$[n, m)$\\
\texttt{X[Y]}

  将\texttt{Y}中所有元素作为\texttt{index}，在\texttt{X}中寻找对应元素替换进张量\texttt{Y}对应位置\\
\texttt{X.transpose(($d_0$, $d_1$, ...))}

  转置张量，改变张量的shape，第一维在原$X$的$d_0$维上遍历元素，第二维在原$X$的$d_1$上遍历\\
\texttt{X.asnumpy()} // 转换成python list\\
\texttt{nd.stack($x_1$, $x_2$, ...)} // 根据提供的子张量，构造高维的张量
\texttt{nd.add\_n($x_1$, $x_2$, ...)} // 多个NDArray相加，等同$x_1 + x_2 + ...$\\
\texttt{func(*[elem, elem, ...])}

  在函数输入中，当函数得到多个元素，调取方法为func(elem, elem, ...)。使用 * 将list转换成分隔的参数输入\\\\
\texttt{from d2lzh import plt}\\
\texttt{plt.scatter(array\_x, array\_y, 1)}

  描点，\texttt{array\_x}, \texttt{array\_y}为python list\\\\
\texttt{nd.save('FILE\_NAME', X)}

  存储NDArray数据，存入FILE\_NAME文件中

  X 可为 NDArray, [NDArray] 数组, {KEY: NDArray, ...} 字典
  
  读取：\texttt{X = load('FILE\_NAME')}。读取 NDArray 和 [NDArray] 时返回类型都为list

% ----------------------------------------------------------------------
% |                               训练                                  |
% ----------------------------------------------------------------------
\section{训练}

\noindent \texttt{from mxnet import autograd}\\
\texttt{x.attach\_grad()} // 为自变量x的 $\frac{d }{d x} $项分配内存\\
\texttt{with autograd.record():}

  \texttt{因变量 = 关于x的表达式}
  
  - 关于x的表达式可以为一个自定义function，不需要是一个连续的数学函数
  
  - 自定义函数必须将所有使用的变量包括在def输入变量中，不可使用全局变量。const仍可使用全局变量
  
  - 当变量为另一函数的结果，计算另一函数的步骤需放进with 中，不能在with scope外计算完with内调取\\
\texttt{因变量.backward()}
  
  - 定义x的表达式，并计算表达式在x内每一元素值上的斜率，对应斜率矩阵存在x.attach\_grad()分配的内存中
  
  - 当使用多组sample，因变量为向量。此时backward等同于\texttt{因变量.sum().backward()}\\
\texttt{x.grad} // 调取斜率矩阵\\
\texttt{autograd.is\_training()} // 在\texttt{autograd.record()内返回true，否则返回false}

% ----------------------------------------------------------------------
% |                     使用neural network模型训练                       |
% ----------------------------------------------------------------------
\section{使用neural network模型训练}

\noindent \texttt{from mxnet.gluon import nn}\\
\texttt{from mxnet import init}\\
\texttt{from mxnet.gluon import loss as gloss}\\
\texttt{net = nn.Dense(2, IN\_UNITS*, ACTIVATION*)}

  创建一个全连接层，包含2个节点

  IN\_UNITS = \texttt{in\_units=N} 避免延后初始化。定义输入变量数，使得net在调用initialize()后即有权重矩阵，否则需要一次forward后才能访问权重矩阵
  
  ACTIVATION = \texttt{activation='relu'} 定义激发函数\\
\texttt{net = nn.Sequential()} // 创建一个神经网络模型，不包含任何layer\\
\texttt{net.add(nn.Dense(...), nn.Dropout(PROB), ...))}

  add中可同时包含多个层
  
  nn.Dropout()为丢弃法使用的丢弃层。定义前一层的权重有PROB几率被清零，1-p几率被拉伸\\
\texttt{net.initialize(INIT, FORCE\_REINIT*)} 

  初始化层内的参数，随后调用\texttt{net(X)}得到全连接层对输入矩阵X的输出
  
  INIT = \texttt{init.Normal(sigma=0.3)} 初始化整个神经网络，对每一层调用initialize。所有权重$\sim N(0, 0.3)$，所有偏差值=0

  INIT = \texttt{init.Xavier()} 使用Xavier随机初始化
  
  FORCE\_INIT = \texttt{force\_init = True} 强制初始化参数，不论参数有没有被初始化过
  
  对nn.Dense() 和nn.Sequential() 创建的net都可调用，用法一样\\
\texttt{net[i].params}

  访问第i hidden layer的权重 偏差值。
  
  返回类型为ParameterDict，可通过['KEY']分别得到权重和偏差\\
\texttt{net[i].weight} 
  
  直接访问权重

  net[i].weight.data() 得到NDArray类型的权重矩阵\\
\texttt{net[i].bias} // 得到偏差值，\texttt{.data()}得到NDArray矩阵\\
\texttt{loss = gloss.L2Loss()}
  
  定义损失函数为平方损失函数，loss为一函数\\
\texttt{trainer = gluon.Trainer(net.collect\_params(NAME), 'sgd', \{'learning\_rate': 0.03\}, WD)}

  定义每一步优化函数，使用sgd梯度下降
  
  NAME无定义，则同时训练权重
  
  \quad NAME = \texttt{'.*weight'}，只训练权重 
  
  \quad NAME = \texttt{'.*bias'}，只训练偏差
    
  WD无定义，则不使用权重衰减
  
  \quad WD = \texttt{wd:'wd'}，使用权重衰减\\
\texttt{dataset = gdata.ArrayDataset(features, labels)}\\
\texttt{data\_iter = gdata.DataLoader(dataset, batch\_size, shuffle=True, last\_batch='discard', num\_workers)}

  按批量读取数据，\texttt{num\_workers}代表使用的额外处理器数，0代表没有额外处理器
  
  last\_batch='discard' 当不能将数据集等分进批量中时，忽略余下数据集\\
\texttt{trainer.step(batch\_size)}

  调用优化函数，取batch\_size个sample做一步训练\\
\texttt{l = loss(net(features), labels)}

  调用loss函数

% ----------------------------------------------------------------------
% |                   自定义class形式的neural network                    |
% ----------------------------------------------------------------------
\section{自定义class形式的neural network}
\noindent \texttt{class CLASS\_NAME(nn.Block):}

  \texttt{def \_\_init\_\_(self, **kwargs):}

  \quad \texttt{super(NySequential, self).\_\_init\_\_(**kwargs)}

  // 初始化class\\

  \quad \texttt{self.weight = self.params.get('weight', shape=(., .))}

  // 通过系统定义的params生成类型为parameterDict的参数矩阵\\

  \texttt{def add(self, block):}

  \quad \texttt{self.\_children[block.KEY] = block}

  // 加入一个layer，使用layer的一个key做index标注此layer存在哪一层

  // 例：layer为nn.Dense(2, activation='relu')\\

  \texttt{def forward(self, x):}

  \quad \texttt{for block in self.\_children.values():}

  \quad \quad \texttt{x = block(x)}

  \quad \quad \texttt{return x}

  // forward写法，调用net(X)等同于调用net.forward(X)\\
\texttt{class INIT\_NAME(init.Initializer):}

  \texttt{def \_init\_weight(self, name, data):}

  \quad \texttt{(对data更改值的函数)}\\
\texttt{net.initialize(INIT\_NAME)}

  使用定义的函数初始化权重 偏差

% ----------------------------------------------------------------------
% |                     图像分类数据集fashion-MNIST                      |
% ----------------------------------------------------------------------
\section{图像分类数据集fashion-MNIST}
\noindent \texttt{from mxnet.gluon import data as gdata}\\
\texttt{mnist\_train = gdata.vision.FashionMNIST(train=True)} 

  得到数据集, \texttt{train=True}为训练数据集，否则为测试训练集\\
\texttt{features, label = mnist\_train[i:j]}
  
  得到一个或多个sample
  
  features为(j-i, 28, 28, 1)的张量，类型为\texttt{NDArray}\\
\texttt{from d2lzh import show\_fashion\_mnist}\\
\texttt{show\_fashion\_mnist(features, ['label1', 'label2'])}

  打印图片，显示图片仍使用plt.show()

  features必须包含多余一个图片矩阵，否则报错

% ----------------------------------------------------------------------
% |                            图片显示 操作                                 |
% ----------------------------------------------------------------------
\section{图片显示\ 操作}
\noindent \texttt{from mxnet import image}\\
\texttt{d2lzh.set\_figsize()} // 定义画布为400 x 500\ 画布为plt\\
\texttt{img = image.imread('FILE\_PATH')} // 定义图片\\
\texttt{fig = d2lzh.plt.imshow(img.asnumpy())} // 显示图片\\
\texttt{aux = gdata.vision.transforms.}

  \texttt{RandomFlipLeftRight()} //0.5\ 几率对图片左右翻转

  \texttt{RandomFlipTopBottom()} //0.5\ 几率上下翻转

  \texttt{RandomResizedCrop(($WIDTH$, $HEIGHT$), $SCALE$, $RATIO$)}

  \quad $WIDTH$, $HEIGHT$定义输出图片的\ *宽高*
    
  \quad \texttt{scale=($S_{min}$, $S_{max}$)} 0 < $S_{min}$, $S_{max}$ <= 1\ 输出图像面积最小为原图$S_{min}$, 最大原图$S_{max}$ 
    
  \quad \texttt{ratio=($R_{min}$, $R_{max}$)} $R_{min}$\ <\ 输出图像\ 宽高比值\ <\ $R_{min}$
  
  \texttt{RandomBrightness(P)} 

  \quad 调整亮度，新亮度在\ 原亮度(1 $\pm$ P)范围内

  \texttt{RandomHue(0.5)} // 调整色调

  \texttt{RandomColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)}

  \quad 同时调整亮度\ 对比度\ 饱和度\ 色调
  
  \texttt{Compose([AUX1, AUX2, ...])} // 叠加多个转换\\
\texttt{img = aux(img)} // 使用aux\\
\texttt{rect = d2lzh.plt.Rectangle(xy=(X, Y), WIDTH, HEIGHT, FILL, EDGECOLOUR, LINEWIDTH)}\\
\texttt{fig.axes.add\_patch(rect)}
  
  在图像上添加矩形框

  \texttt{FILL}为\texttt{boolean}值，图像是否填充

  \texttt{EDGECOLOUR}为string，代表边框颜色\\
\texttt{from mxnet import contrib}\\
\texttt{Y = contrib.nd.MultiBoxPrior(NDArray, SIZES, RATIOS)}

  SIZES = \texttt{sizes=}$(s_1, s_2,...,s_n)$python数组

  RATIOS = \texttt{ratios=}$r_1, r_2, ...,r_m$python数组

  目的：
  
  \quad 得到坐标代表锚框，包括$(s_1, r_1), (s_1, r_2), ..., (s_1, r_m), (s_2, r_1), ..., (s_n, r_m)$共$n+m-1$个锚框类型

  \quad reshape成(高，宽，-1，4)

  \quad 4代表每一锚框由左上和右下坐标表示，坐标为占宽高比，非像素值

  \quad \quad 此时访问[h, w, :, 4]得到中心点在(h, w)的所有锚框坐标\\
\texttt{show\_bboxes(AXES, BBOXES, LABELS, COLORS)}

  AXES = \texttt{fig.axes}\ 传入添加进的图片axes

  BBOXES (n, 4)形状NDArray，每一4元素NDArray表示一锚框右上左下坐标

  LABELS python string数组，长度为\texttt{n}，对应BBOXES个数

  COLORS = \texttt{'k'} 颜色为黑，为空则使用默认不同颜色\\
\texttt{show\_images(IMGS, Y, X)}

  同时显示多个图片，IMGS为一python数组的img

  Y X指定输出的图片行数\ 列数，\textbf{X Y都不为1，否则无法创建subplot，报错}\\
\texttt{feature, label = voc\_rand\_crop(FEATURE, LABEL, HEIGHT, WIDTH)}

  \texttt{FEATURE} \texttt{LABEL}都为img

  随机选取\texttt{FEATURE}和\texttt{LABEL}中\texttt{(HEIGHT, WIDTH)}大小的一部分像素，作为\texttt{feature} \texttt{label}两个img返回
  
  \texttt{feature} \texttt{label}坐标位置相同，如分别为\texttt{FEATURE} \texttt{LABEL} \texttt{(10, 20)}像素位置开始的图片\\
\texttt{voc\_img\_set = VOCSegDataset(IS\_TRAIN, CROP\_SIZE, VOC\_DIR, COLORMAP2LABEL)}

  将VOC图像集合封装进python类，\texttt{VOCSegDataset} extends \texttt{gdata.DataSet}。可以直接调用\texttt{gdata.DataLoader(voc\_img\_set, ...)}

  IS\_TRAIN = \texttt{is\_train=True} 类包含的是测试还是训练集合

  CROP\_SIZE = \texttt{(h, w)} 图像预处理得到的随机小图大小

  VOC\_DIR = \texttt{'/path...'} VOC2012文件夹所在位置

  COLORMAP2LABEL 为python 1维数组，记录$256^3$种颜色对应的标签index\\
\texttt{conv\_trans = nn.Conv2DTranspose(通道数, kernel\_size=4, padding=1, strides=2)}\\
\texttt{conv]\_trans.initialize(init.Constant(bilinear\_kernel(CHANNEL\_NUM\_IN, CHANNEL\_NUM\_OUT=conv\_trans通道数, kernel\_size=4)))}\\

  初始化转置卷积层
% ----------------------------------------------------------------------
% |              convolutional neural network卷积神经网络                |
% ----------------------------------------------------------------------
\section{convolutional network卷积神经网络}
\noindent \texttt{CONV\_2D = nn.Conv2D(CHANNEL\_NUM, KERNEL\_SIZE, PADDING, STRIDE)}

  - 定义一卷积层

  - CHANNEL\_NUM = 整数，定义输出通道数

  - KERNEL\_SIZE = \texttt{kernel\_size = (., .)} 定义kernel的shape

  \quad \texttt{kernel=N} 等同\texttt{kernel=(N, N)}
  
  - PADDING = \texttt{padding=(n, m)}  输入矩阵上下分别添加n行，左右分别添加m列全零元素

  \quad \texttt{padding=N} 等同\texttt{padding=(N, N)}
  
  - STRIDE = \texttt{stride=(n, m)} 横向移动步幅为n，纵向步幅为m
  
  \quad \texttt{stride=N} 等同\texttt{stride=(N, N)}\\
\texttt{CONV\_2D.weight.data()[:] = ... * CONV\_2D.weight.grad()}

  优化函数写法。若输出矩阵计算由对每一元素赋值得到，无法使用自动求斜率
  
  使用的输入输出矩阵shape：\texttt{(批量大小，通道数，行数，列数)}\\
\texttt{nn.MaxPool2D(SHAPE, PADDING, STRIDE)}

  SHAPE = \texttt{(n, m)} 或 \texttt{N} 定义池化窗口shape

  PADDING, STRIDE同kernel

\end{document}
